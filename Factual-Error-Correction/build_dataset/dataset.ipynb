{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPFL news dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data_epfl_news/news_data_train.csv')\n",
    "df_eng = df[df.language == 'en'].reset_index(drop=True)\n",
    "\n",
    "df = pd.read_csv('../data_epfl_news/news_data_val.csv')\n",
    "df_eng_val = df[df.language == 'en'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preproccess(raw_html):\n",
    "  if len(re.findall('<img(.*?)</img>', raw_html)) != 0:\n",
    "    for im in re.findall('<img(.*?)</img>', raw_html):\n",
    "      im = '<img'+im+'</img>'\n",
    "      raw_html = raw_html.replace(im, '')\n",
    "  if len(re.findall('<a(.*?)</a>', raw_html)) != 0:\n",
    "    for im in re.findall('<a(.*?)</a>', raw_html):\n",
    "      im = '<a'+im+'</a>'\n",
    "      raw_html = raw_html.replace(im, '')\n",
    "  if len(re.findall('<iframe(.*?)</iframe>', raw_html)) != 0:\n",
    "    for im in re.findall('<iframe(.*?)</iframe>', raw_html):\n",
    "      im = '<iframe'+im+'</iframe>'\n",
    "      raw_html = raw_html.replace(im, '')\n",
    "  soup = BeautifulSoup(raw_html)\n",
    "  new_text = soup.get_text('\\n').strip()\n",
    "  return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "summ = []\n",
    "for i in range(df_eng.shape[0]):\n",
    "    docs.append(preproccess(df_eng.document[i]))\n",
    "    summ.append(preproccess(df_eng.summary[i]))\n",
    "\n",
    "df_eng['document_prep'] = docs\n",
    "df_eng['summary_prep'] = summ\n",
    "\n",
    "docs_ = []\n",
    "summ_ = []\n",
    "for i in range(df_eng_val.shape[0]):\n",
    "    docs_.append(preproccess(df_eng_val.document[i]))\n",
    "    summ_.append(preproccess(df_eng_val.summary[i]))\n",
    "\n",
    "df_eng_val['document_prep'] = docs_\n",
    "df_eng_val['summary_prep'] = summ_\n",
    "\n",
    "df_eng_val = df_eng_val[['document', 'summary', 'document_prep', 'summary_prep']].dropna()\n",
    "df_eng = df_eng[['document', 'summary', 'document_prep', 'summary_prep']].dropna()\n",
    "\n",
    "df_eng.to_csv('../data_epfl_news/news_data_train_prep.csv')\n",
    "df_eng_val.to_csv('../data_epfl_news/news_data_val_prep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of corrupted file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = '../data/val-clean.jsonl'\n",
    "file_path_date = '../data/val-dateswp.jsonl'\n",
    "file_path_ent = '../data/val-entswp.jsonl'\n",
    "file_path_num = '../data/val-numswp.jsonl'\n",
    "file_path_pronoun = '../data/val-pronoun.jsonl'\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f, \\\n",
    "     open(file_path_date, 'r', encoding=\"utf-8\") as f_date, \\\n",
    "     open(file_path_ent, 'r', encoding=\"utf-8\") as f_ent, \\\n",
    "     open(file_path_num, 'r', encoding=\"utf-8\") as f_num, \\\n",
    "     open(file_path_pronoun, 'r', encoding=\"utf-8\") as f_pronoun:\n",
    "\n",
    "            data = np.array([json.loads(line) for line in f])\n",
    "            data_date = np.array([json.loads(line) for line in f_date])\n",
    "            data_ent = np.array([json.loads(line) for line in f_ent])\n",
    "            data_num = np.array([json.loads(line) for line in f_num])\n",
    "            data_pronoun = np.array([json.loads(line) for line in f_pronoun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': '(CNN)Singer-songwriter David Crosby hit a jogger with his car Sunday evening, a spokesman said. The accident happened in Santa Ynez, California, near where Crosby lives. Crosby was driving at approximately 50 mph when he struck the jogger, according to California Highway Patrol Spokesman Don Clotworthy. The posted speed limit was 55. The jogger suffered multiple fractures, and was airlifted to a hospital in Santa Barbara, Clotworthy said. His injuries are not believed to be life threatening. \"Mr. Crosby was cooperative with authorities and he was not impaired or intoxicated in any way. Mr. Crosby did not see the jogger because of the sun,\" said Clotworthy. According to the spokesman, the jogger and Crosby were on the same side of the road. Pedestrians are supposed to be on the left side of the road walking toward traffic, Clotworthy said. Joggers are considered pedestrians. Crosby is known for weaving multilayered harmonies over sweet melodies. He belongs to the celebrated rock group Crosby, Stills & Nash. \"David Crosby is obviously very upset that he accidentally hit anyone. And, based off of initial reports, he is relieved that the injuries to the gentleman were not life threatening,\" said Michael Jensen, a Crosby spokesman. \"He wishes the jogger a very speedy recovery.\"',\n",
       " 'summary': 'Accident happens in Santa Ynez, California, near where Crosby lives .\\nThe jogger suffered multiple fractures; his injuries are not believed to be life-threatening .',\n",
       " 'claim': 'Accident happens in Santa Ynez, Crosby, near where Crosby lives .\\nThe jogger suffered multiple fractures; his injuries are not believed to be life-threatening .',\n",
       " 'label': 'INCORRECT',\n",
       " 'extraction_span': None,\n",
       " 'backtranslation': False,\n",
       " 'augmentation': 'EntitySwap',\n",
       " 'augmentation_span': [6, 6],\n",
       " 'noise': False}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix of the dataset to achive 30% of corrupted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\n",
      "Selected data\n",
      "Saving data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1895it [00:00, 6419.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# file_path = '../data/train-clean.jsonl'\n",
    "# file_path_date = '../data/train-dateswp.jsonl'\n",
    "# file_path_ent = '../data/train-entswp.jsonl'\n",
    "# file_path_num = '../data/train-numswp.jsonl'\n",
    "# file_path_pronoun = '../data/train-pronoun.jsonl'\n",
    "# output_file = \"../data/train.jsonl\"\n",
    "\n",
    "file_path = '../data_epfl_news/train-clean.jsonl'\n",
    "file_path_date = '../data_epfl_news/train-dateswp.jsonl'\n",
    "file_path_ent = '../data_epfl_news/train-entswp.jsonl'\n",
    "file_path_num = '../data_epfl_news/train-numswp.jsonl'\n",
    "file_path_pronoun = '../data_epfl_news/train-pronoun.jsonl'\n",
    "output_file = \"../data_epfl_news/train.jsonl\"\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f, \\\n",
    "     open(file_path_date, 'r', encoding=\"utf-8\") as f_date, \\\n",
    "     open(file_path_ent, 'r', encoding=\"utf-8\") as f_ent, \\\n",
    "     open(file_path_num, 'r', encoding=\"utf-8\") as f_num, \\\n",
    "     open(file_path_pronoun, 'r', encoding=\"utf-8\") as f_pronoun, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as fd:\n",
    "\n",
    "            data = np.array([json.loads(line) for line in f])\n",
    "            data_date = np.array([json.loads(line) for line in f_date])\n",
    "            data_ent = np.array([json.loads(line) for line in f_ent])\n",
    "            data_num = np.array([json.loads(line) for line in f_num])\n",
    "            data_pronoun = np.array([json.loads(line) for line in f_pronoun])\n",
    "\n",
    "            print('Loaded data')\n",
    "\n",
    "            # num_clean = np.random.choice(len(data)-1, size=201644, replace=False)\n",
    "            # num_date = np.random.choice(len(data_date)-1, size=16858, replace=False)\n",
    "            # num_ent = np.random.choice(len(data_ent)-1, size=35113, replace=False)\n",
    "            # num_num = np.random.choice(len(data_num)-1, size=13408, replace=False)\n",
    "            # num_pronoun = np.random.choice(len(data_pronoun)-1, size=20204, replace=False)\n",
    "\n",
    "            num_clean = np.random.choice(len(data)-1, size=1325, replace=False) #1892\n",
    "            num_date = np.random.choice(len(data_date)-1, size=114, replace=False) #625\n",
    "            num_ent = np.random.choice(len(data_ent)-1, size=228, replace=False) #1775\n",
    "            num_num = np.random.choice(len(data_num)-1, size=95, replace=False) #550\n",
    "            num_pronoun = np.random.choice(len(data_pronoun)-1, size=133, replace=False) #875\n",
    "\n",
    "            data = data[num_clean]\n",
    "            data_date = data_date[num_date]\n",
    "            data_ent = data_ent[num_ent]\n",
    "            data_num = data_num[num_num]\n",
    "            data_pronoun = data_pronoun[num_pronoun]\n",
    "\n",
    "            print('Selected data')\n",
    "\n",
    "            data = np.concatenate((data, data_date, data_ent, data_num, data_pronoun))\n",
    "            np.random.shuffle(data)\n",
    "\n",
    "            print('Saving data')\n",
    "\n",
    "            for i, example in tqdm(enumerate(data)):\n",
    "                example[\"id\"] = i\n",
    "                fd.write(json.dumps(example, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\n",
      "Saving data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "733it [00:00, 7128.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# file_path = '../data/val-clean.jsonl'\n",
    "# file_path_date = '../data/val-dateswp.jsonl'\n",
    "# file_path_ent = '../data/val-entswp.jsonl'\n",
    "# file_path_num = '../data/val-numswp.jsonl'\n",
    "# file_path_pronoun = '../data/val-pronoun.jsonl'\n",
    "# output_file = \"../data/val.jsonl\"\n",
    "\n",
    "file_path = '../data_epfl_news/val-clean.jsonl'\n",
    "file_path_date = '../data_epfl_news/val-dateswp.jsonl'\n",
    "file_path_ent = '../data_epfl_news/val-entswp.jsonl'\n",
    "file_path_num = '../data_epfl_news/val-numswp.jsonl'\n",
    "file_path_pronoun = '../data_epfl_news/val-pronoun.jsonl'\n",
    "output_file = \"../data_epfl_news/val.jsonl\"\n",
    "\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f, \\\n",
    "     open(file_path_date, 'r', encoding=\"utf-8\") as f_date, \\\n",
    "     open(file_path_ent, 'r', encoding=\"utf-8\") as f_ent, \\\n",
    "     open(file_path_num, 'r', encoding=\"utf-8\") as f_num, \\\n",
    "     open(file_path_pronoun, 'r', encoding=\"utf-8\") as f_pronoun, \\\n",
    "     open(output_file, \"w\", encoding=\"utf-8\") as fd:\n",
    "\n",
    "            data = np.array([json.loads(line) for line in f])\n",
    "            data_date = np.array([json.loads(line) for line in f_date])\n",
    "            data_ent = np.array([json.loads(line) for line in f_ent])\n",
    "            data_num = np.array([json.loads(line) for line in f_num])\n",
    "            data_pronoun = np.array([json.loads(line) for line in f_pronoun])\n",
    "\n",
    "            print('Loaded data')\n",
    "\n",
    "            data = np.concatenate((data, data_date, data_ent, data_num, data_pronoun))\n",
    "            np.random.shuffle(data)\n",
    "\n",
    "            print('Saving data')\n",
    "\n",
    "            for i, example in tqdm(enumerate(data)):\n",
    "                example[\"id\"] = i\n",
    "                fd.write(json.dumps(example, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
