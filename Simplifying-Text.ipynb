{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175b8585-4037-42c7-a96c-a4892f30e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.utils\n",
    "# Import relevant libraries\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def create_experiment_dir(repo_dir):\n",
    "    \"\"\"\n",
    "    Create a unique experiment directory based on the current timestamp.\n",
    "\n",
    "    Args:\n",
    "        repo_dir: The repository directory location\n",
    "\n",
    "    Returns:\n",
    "        Path: The created experiment directory path\n",
    "    \"\"\"\n",
    "    dir_name = '{}'.format(int(time.time() * 1000000))\n",
    "    path = Path(repo_dir) / f\"exp_{dir_name}\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def log_parameters(filepath, parameters):\n",
    "    \"\"\"\n",
    "    Log parameters to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the JSON file\n",
    "        parameters: Parameters to log\n",
    "    \"\"\"\n",
    "    with filepath.open('w') as f:\n",
    "        json.dump({k: str(v) for k, v in parameters.items()}, f, indent=4)\n",
    "\n",
    "\n",
    "def save_log(output_dir, model_name, epoch, loss=None, sari=None, data_type='train'):\n",
    "    \"\"\"\n",
    "    Save log for training or validation data.\n",
    "\n",
    "    Args:\n",
    "        output_dir: output directory\n",
    "        model_name: use model name to save\n",
    "        epoch (int): Current epoch.\n",
    "        loss (float): Loss value (optional).\n",
    "        sari (float): SARI score (optional).\n",
    "        data_type: Data type train or validation\n",
    "    \"\"\"\n",
    "    if data_type == 'train':\n",
    "        with open('{}/{}_training_log.csv'.format(output_dir, model_name.replace(\"/\", \"-\")), 'a') as f:\n",
    "            log_line = f\"{epoch},{loss if loss is not None else ''}\\n\"\n",
    "            f.write(log_line)\n",
    "    elif data_type == 'validation':\n",
    "        with open('{}/{}_validation_log.csv'.format(output_dir, model_name.replace(\"/\", \"-\")), 'a') as f:\n",
    "            log_line = f\"{epoch},{loss if loss is not None else ''},{sari if sari is not None else ''}\\n\"\n",
    "            f.write(log_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3887712-7e66-4013-99ab-7ccdbf0a6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing\n",
    "\n",
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def yield_lines(filepath):\n",
    "    \"\"\"\n",
    "    Generator function to yield lines from a file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str or Path): Path to the file.\n",
    "\n",
    "    Yields:\n",
    "        str: Each line from the file, stripped of trailing whitespace.\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    with filepath.open('r') as f:\n",
    "        for line in f:\n",
    "            yield line.rstrip()\n",
    "\n",
    "\n",
    "def read_lines(filepath):\n",
    "    \"\"\"\n",
    "    Reads all lines from a file and returns them as a list.\n",
    "\n",
    "    Args:\n",
    "        filepath (str or Path): Path to the file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of lines from the file, each stripped of trailing whitespace.\n",
    "    \"\"\"\n",
    "    return [line.rstrip() for line in yield_lines(filepath)]\n",
    "\n",
    "\n",
    "def get_data_filepath(data_set_dir, dataset, phase, data_type, i=None):\n",
    "    \"\"\"\n",
    "    Constructs the file path for a dataset file based on provided parameters.\n",
    "\n",
    "    Args:\n",
    "        data_set_dir (str or Path): Directory containing datasets.\n",
    "        dataset (str): Name of the dataset.\n",
    "        phase (str): Phase of the data (e.g., 'train' or 'valid').\n",
    "        data_type (str): Type of data (e.g., 'complex' or 'simple').\n",
    "        i (int, optional): Optional index to append as a suffix to the filename.\n",
    "\n",
    "    Returns:\n",
    "        Path: Constructed file path as a Path object.\n",
    "    \"\"\"\n",
    "    suffix = f'.{i}' if i is not None else ''\n",
    "    data_filename = f'{dataset}.{phase}.{data_type}{suffix}'\n",
    "    return Path(data_set_dir) / dataset / data_filename\n",
    "\n",
    "\n",
    "def generate_hash(data):\n",
    "    h = hashlib.new('md5')\n",
    "    h.update(str(data).encode())\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def count_line(filepath):\n",
    "    filepath = Path(filepath)\n",
    "    line_count = 0\n",
    "    with filepath.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            line_count += 1\n",
    "    return line_count\n",
    "\n",
    "\n",
    "def write_lines(lines, filepath):\n",
    "    filepath = Path(filepath)\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with filepath.open(\"w\") as fout:\n",
    "        for line in lines:\n",
    "            fout.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62c1d59-8d69-4922-a72a-d1f86a46963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from util.train_valid_data_generation\n",
    "\n",
    "# Import libraries\n",
    "from torch.utils.data import Dataset\n",
    "# from util.processing.preprocessor import (\n",
    "#     yield_lines, read_lines, get_data_filepath\n",
    "# )\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_set_dir, dataset, tokenizer, max_len=256, sample_size=1):\n",
    "        \"\"\"\n",
    "        Initializes the training dataset.\n",
    "\n",
    "        Args:\n",
    "            data_set_dir: Path to data\n",
    "            dataset: Name of the dataset.\n",
    "            tokenizer: Tokenizer object to tokenize the data.\n",
    "            max_len (int): Maximum length of the tokenized sequences.\n",
    "            sample_size (float): Fraction of the dataset to sample.\n",
    "        \"\"\"\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        print(\"Initializing TrainDataset...\")\n",
    "        self.source_filepath = get_data_filepath(data_set_dir, dataset, 'train', 'complex')\n",
    "        self.target_filepath = get_data_filepath(data_set_dir, dataset, 'train', 'simple')\n",
    "        print(\"Dataset paths initialized.\")\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Loads the source and target data.\"\"\"\n",
    "        self.inputs = read_lines(self.source_filepath)\n",
    "        self.targets = read_lines(self.target_filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset based on the sample size.\"\"\"\n",
    "        return int(len(self.inputs) * self.sample_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Fetches a single item from the dataset.\"\"\"\n",
    "        source = self.inputs[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            [source],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_targets = self.tokenizer(\n",
    "            [target],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        source_ids = tokenized_inputs[\"input_ids\"].squeeze()\n",
    "        target_ids = tokenized_targets[\"input_ids\"].squeeze()\n",
    "        src_mask = tokenized_inputs[\"attention_mask\"].squeeze()\n",
    "        target_mask = tokenized_targets[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids, \n",
    "            \"source_mask\": src_mask, \n",
    "            \"target_ids\": target_ids, \n",
    "            \"target_mask\": target_mask,\n",
    "            \"sources\": source, \n",
    "            \"targets\": [target],\n",
    "            \"source\": source, \n",
    "            \"target\": target\n",
    "        }\n",
    "\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, data_set_dir, dataset, tokenizer, max_len=256, sample_size=1):\n",
    "        \"\"\"\n",
    "        Initializes the validation dataset.\n",
    "\n",
    "        Args:\n",
    "            data_set_dir: Path to data\n",
    "            dataset: Name or path of the dataset.\n",
    "            tokenizer: Tokenizer object to tokenize the data.\n",
    "            max_len (int): Maximum length of the tokenized sequences.\n",
    "            sample_size (float): Fraction of the dataset to sample.\n",
    "        \"\"\"\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        print(\"Initializing ValDataset...\")\n",
    "        self.source_filepath = get_data_filepath(data_set_dir, dataset, 'valid', 'complex')\n",
    "        self.target_filepaths = get_data_filepath(data_set_dir, dataset, 'valid', 'simple')\n",
    "        print(\"Dataset paths initialized.\")\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Loads the source and target data.\"\"\"\n",
    "        self.inputs = [line for line in yield_lines(self.source_filepath)]\n",
    "        self.targets = [line for line in yield_lines(self.target_filepaths)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset based on the sample size.\"\"\"\n",
    "        return int(len(self.inputs) * self.sample_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Fetches a single item from the dataset.\"\"\"\n",
    "        return {\n",
    "            \"source\": self.inputs[index], \n",
    "            \"targets\": self.targets[index]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fccdf2a-45b3-48c2-adbe-00f84ee7ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate_model.evaluation metrics\n",
    "\n",
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import textstat\n",
    "# from util.processing.preprocessor import get_data_filepath\n",
    "from easse.sari import corpus_sari as easse_corpus_sari\n",
    "from easse.fkgl import corpus_fkgl as easse_corpus_fkgl\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_dataset(dataset_dir, dataset_name, phase='test', percentage=1.0):\n",
    "    \"\"\"\n",
    "    Load the dataset for evaluation with an optional parameter to specify the percentage of data to be used.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str or Path): Path to the dataset directory.\n",
    "        dataset_name (str): Name of the dataset (e.g., 'dwiki' or 'wiki_doc').\n",
    "        phase (str): Dataset phase to load ('train', 'valid', 'test').\n",
    "        percentage (float): Percentage of data to be used (value between 0.0 and 1.0).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of complex sentences, list of simple sentences)\n",
    "    \"\"\"\n",
    "    complex_filepath = get_data_filepath(dataset_dir, dataset_name, phase, 'complex')\n",
    "    simple_filepath = get_data_filepath(dataset_dir, dataset_name, phase, 'simple')\n",
    "\n",
    "    # Read lines from files\n",
    "    complex_sents = Path(complex_filepath).read_text().splitlines()\n",
    "    simple_sents = Path(simple_filepath).read_text().splitlines()\n",
    "\n",
    "    # Use the specified percentage of the data\n",
    "    data_size = len(complex_sents)\n",
    "    selected_size = int(data_size * percentage)\n",
    "\n",
    "    # Randomly select the data subset\n",
    "    indices = list(range(data_size))\n",
    "    random.shuffle(indices)\n",
    "    selected_indices = indices[:selected_size]\n",
    "\n",
    "    complex_sents = [complex_sents[i] for i in selected_indices]\n",
    "    simple_sents = [simple_sents[i] for i in selected_indices]\n",
    "\n",
    "    return complex_sents, simple_sents\n",
    "\n",
    "\n",
    "\n",
    "class BartModelEvaluator:\n",
    "    \"\"\"\n",
    "    A class for evaluating a BART-based summarization model using SARI, D-SARI, and FKGL metrics.\n",
    "\n",
    "    Args:\n",
    "        model_config : Configuration dictionary containing the device to run the model on (\"cuda\", \"cpu\", or \"mps\").\n",
    "        model (BartForConditionalGeneration): Pre-trained BART model to evaluate.\n",
    "        tokenizer (BartTokenizer): Tokenizer for the BART model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_config, model, tokenizer):\n",
    "        self.model = model.to(model_config['device'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model_config['device']\n",
    "        self.max_seq_length = model_config['max_seq_length']\n",
    "        self.output_location = model_config['output_dir']\n",
    "\n",
    "    def generate_summary(self, sentence, max_length=256):\n",
    "        \"\"\"\n",
    "        Generate a summary for a given input sentence using the BART model.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence to be summarized.\n",
    "            max_length (int): Maximum length of the generated summary.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated summary.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        summary_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sari_and_d_sari(source_sent, predicted_sent, references):\n",
    "        \"\"\"\n",
    "        Calculate SARI and D-SARI scores for text simplification.\n",
    "\n",
    "        Args:\n",
    "            source_sent (str): Source sentence.\n",
    "            predicted_sent (str): Predicted simplified sentence.\n",
    "            references (list): List of reference simplified sentences.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (SARI score, D-SARI score)\n",
    "        \"\"\"\n",
    "        source_tokens = set(word_tokenize(source_sent))\n",
    "        predicted_tokens = set(word_tokenize(predicted_sent))\n",
    "        reference_tokens = [set(word_tokenize(ref)) for ref in references]\n",
    "\n",
    "        # Calculate addition, deletion, and keep scores\n",
    "        add_scores = [\n",
    "            len(predicted_tokens - ref) / max(1, len(predicted_tokens))\n",
    "            for ref in reference_tokens\n",
    "        ]\n",
    "        keep_scores = [\n",
    "            len(predicted_tokens & ref) / max(1, len(ref))\n",
    "            for ref in reference_tokens\n",
    "        ]\n",
    "        delete_score = len(source_tokens - predicted_tokens) / max(1, len(source_tokens))\n",
    "\n",
    "        sari = (sum(add_scores) + sum(keep_scores) + delete_score) / (len(add_scores) + len(keep_scores) + 1)\n",
    "        d_sari = delete_score  # D-SARI focuses specifically on the deletion component\n",
    "\n",
    "        return sari, d_sari\n",
    "\n",
    "    def calculate_fkgl(self, text):\n",
    "        \"\"\"\n",
    "        Calculate the Flesch-Kincaid Grade Level (FKGL) score.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            float: FKGL score.\n",
    "        \"\"\"\n",
    "        return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    def evaluate(self, source_sentences, reference_sentences):\n",
    "        \"\"\"\n",
    "        Evaluate a set of source and reference sentences using SARI, D-SARI, and FKGL metrics.\n",
    "\n",
    "        Args:\n",
    "            source_sentences (list): List of source sentences to be simplified.\n",
    "            reference_sentences (list): List of corresponding reference sentences.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing average SARI, D-SARI, and FKGL scores.\n",
    "        \"\"\"\n",
    "        total_sari, total_d_sari, total_fkgl = 0, 0, 0\n",
    "        predictions = []\n",
    "        metrics = []\n",
    "\n",
    "        for i, source_sent in enumerate(source_sentences):\n",
    "            try:\n",
    "                predicted_sent = self.generate_summary(source_sent)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating summary for sample {i}: {e}\")\n",
    "                predicted_sent = \"\"  # Fallback to an empty prediction\n",
    "\n",
    "            predictions.append(predicted_sent)\n",
    "            references = [reference_sentences[i]]  # Assuming one reference per source\n",
    "\n",
    "            # Calculate SARI and D-SARI scores\n",
    "            sari, d_sari = self.calculate_sari_and_d_sari(source_sent, predicted_sent, references)\n",
    "            total_sari += sari\n",
    "            total_d_sari += d_sari\n",
    "\n",
    "            # Calculate FKGL score\n",
    "            fkgl = self.calculate_fkgl(predicted_sent)\n",
    "            total_fkgl += fkgl\n",
    "\n",
    "            # Calculate EASSE SARI and FKGL for this sample\n",
    "            try:\n",
    "                easse_sari = easse_corpus_sari(orig_sents=[source_sent], sys_sents=[predicted_sent],\n",
    "                                               refs_sents=[references])\n",
    "                easse_fkgl = easse_corpus_fkgl([predicted_sent])\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating EASSE metrics for sample {i}: {e}\")\n",
    "                easse_sari = 0\n",
    "                easse_fkgl = 0\n",
    "\n",
    "            # Print metrics for the sample\n",
    "            print(f\"Sample {i + 1}/{len(source_sentences)}\")\n",
    "            print(f\"Source: {source_sent}\")\n",
    "            print(f\"Predicted: {predicted_sent}\")\n",
    "            print(f\"Reference: {references[0]}\")\n",
    "            print(f\"SARI: {sari:.2f}, D-SARI: {d_sari:.2f}, FKGL: {fkgl:.2f}\")\n",
    "            print(f\"EASSE SARI: {easse_sari:.2f}, EASSE FKGL: {easse_fkgl:.2f}\\n\")\n",
    "\n",
    "            # Store metrics in a dictionary for each sample\n",
    "            metrics.append({\n",
    "                'Sample': i + 1,\n",
    "                'Source': source_sent,\n",
    "                'Predicted': predicted_sent,\n",
    "                'Reference': references[0],\n",
    "                'SARI': sari,\n",
    "                'D-SARI': d_sari,\n",
    "                'FKGL': fkgl,\n",
    "                'EASSE SARI': easse_sari,\n",
    "                'EASSE FKGL': easse_fkgl\n",
    "            })\n",
    "\n",
    "        # Calculate average scores\n",
    "        avg_sari = total_sari / len(source_sentences)\n",
    "        avg_d_sari = total_d_sari / len(source_sentences)\n",
    "        avg_fkgl = total_fkgl / len(source_sentences)\n",
    "\n",
    "        # Calculate EASSE SARI and FKGL scores for all predictions\n",
    "        try:\n",
    "            easse_sari = easse_corpus_sari(orig_sents=source_sentences, sys_sents=predictions,\n",
    "                                           refs_sents=[reference_sentences])\n",
    "            easse_fkgl = easse_corpus_fkgl(predictions)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating EASSE metrics for all predictions: {e}\")\n",
    "            easse_sari = 0\n",
    "            easse_fkgl = 0\n",
    "\n",
    "        print(f\"Average SARI: {avg_sari:.2f}\")\n",
    "        print(f\"Average D-SARI: {avg_d_sari:.2f}\")\n",
    "        print(f\"Average FKGL: {avg_fkgl:.2f}\")\n",
    "        print(f\"EASSE SARI: {easse_sari:.2f}\")\n",
    "        print(f\"EASSE FKGL: {easse_fkgl:.2f}\")\n",
    "\n",
    "        # Append average metrics as a separate row\n",
    "        metrics.append({\n",
    "            'Sample': 'Average',\n",
    "            'Source': 'N/A',\n",
    "            'Predicted': 'N/A',\n",
    "            'Reference': 'N/A',\n",
    "            'SARI': avg_sari,\n",
    "            'D-SARI': avg_d_sari,\n",
    "            'FKGL': avg_fkgl,\n",
    "            'EASSE SARI': easse_sari,\n",
    "            'EASSE FKGL': easse_fkgl\n",
    "        })\n",
    "\n",
    "        # Save metrics to a CSV file using pandas\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv('{}/evaluation_metrics_baseline.csv'.format(self.output_location), index=False)\n",
    "\n",
    "        return {\n",
    "            \"SARI\": avg_sari,\n",
    "            \"D-SARI\": avg_d_sari,\n",
    "            \"FKGL\": avg_fkgl,\n",
    "            \"EASSE SARI\": easse_sari,\n",
    "            \"EASSE FKGL\": easse_fkgl\n",
    "        }, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785b1a69-9ea8-4de3-87f4-5569107d8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_models.baseline_model\n",
    "\n",
    "# Import necessary libraries\n",
    "from torch.utils.data import DataLoader\n",
    "# from util.train_valid_data_generation import TrainDataset, ValDataset\n",
    "import pytorch_lightning as pl\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    ")\n",
    "from easse.sari import corpus_sari\n",
    "# from util.utils import save_log\n",
    "\n",
    "\n",
    "class Seq2SeqFineTunedModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A generic PyTorch Lightning module for fine-tuning a sequence-to-sequence model for summarization or text simplification.\n",
    "\n",
    "    Args:\n",
    "        training_parameters (dict): Dictionary of training parameters.\n",
    "        model_name (str): Pre-trained model to fine-tune (e.g., 't5-base', 'Yale-LILY/brio-cnndm-uncased').\n",
    "    \"\"\"\n",
    "    def __init__(self, training_parameters, model_name='t5-base'):\n",
    "        super(Seq2SeqFineTunedModel, self).__init__()\n",
    "\n",
    "        # Store hyperparameters and initialize model and tokenizer\n",
    "        self.save_hyperparameters()\n",
    "        self.training_parameters = training_parameters\n",
    "        self.device_name = training_parameters['device']\n",
    "\n",
    "        # Initialize parameters from the training dictionary\n",
    "        self.model_name = training_parameters['model_name']\n",
    "        self.train_batch_size = training_parameters['train_batch_size']\n",
    "        self.valid_batch_size = training_parameters['valid_batch_size']\n",
    "        self.learning_rate = training_parameters['learning_rate']\n",
    "        self.max_seq_length = training_parameters['max_seq_length']\n",
    "        self.adam_epsilon = training_parameters['adam_epsilon']\n",
    "        self.weight_decay = training_parameters['weight_decay']\n",
    "        self.warmup_steps = training_parameters['warmup_steps']\n",
    "        self.train_sample_size = training_parameters['train_sample_size']\n",
    "        self.valid_sample_size = training_parameters['valid_sample_size']\n",
    "        self.num_train_epochs = training_parameters['num_train_epochs']\n",
    "        self.gradient_accumulation_steps = training_parameters['gradient_accumulation_steps']\n",
    "        self.custom_loss = training_parameters.get('custom_loss', False)\n",
    "        self.scheduler_type = training_parameters.get('scheduler_type', 'linear')\n",
    "        with open('{}/{}_training_log.csv'.format(\n",
    "                self.training_parameters['output_dir'],\n",
    "                self.training_parameters['model_name'].replace(\"/\", \"-\")\n",
    "        ), 'w') as f: f.write('epoch,loss\\n')\n",
    "        with open('{}/{}_validation_log.csv'.format(\n",
    "                training_parameters['output_dir'],\n",
    "                self.training_parameters['model_name'].replace(\"/\", \"-\")\n",
    "        ), 'w') as f: f.write('epoch,loss,sari\\n')\n",
    "\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Data and output paths\n",
    "        self.dataset = self.training_parameters['dataset']\n",
    "        self.data_location = self.training_parameters['data_location']\n",
    "        self.model_store_path = training_parameters['output_dir'] / (model_name + '_fine_tuned')\n",
    "\n",
    "    def is_logger(self):\n",
    "        \"\"\"\n",
    "        Returns True if this is the first rank (for distributed training), False otherwise.\n",
    "        \"\"\"\n",
    "        return self.trainer.global_rank <= 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
    "                decoder_attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (tensor): Input tensor containing tokenized input IDs.\n",
    "            attention_mask (tensor): Attention mask for the input.\n",
    "            decoder_input_ids (tensor): Decoder input IDs for sequence generation.\n",
    "            decoder_attention_mask (tensor): Attention mask for the decoder.\n",
    "            labels (tensor): Target labels for training.\n",
    "\n",
    "        Returns:\n",
    "            ModelOutput: Model's output, including loss if labels are provided.\n",
    "        \"\"\"\n",
    "        return self.model(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=decoder_input_ids,\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a training step, computes loss, and logs the results.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): Batch of training data.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Loss value for the current batch.\n",
    "        \"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        labels = batch['target_ids']\n",
    "\n",
    "        # Ignore padding tokens in loss calculation\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(input_ids=batch[\"source_ids\"],\n",
    "                       attention_mask=batch[\"source_mask\"],\n",
    "                       labels=labels,\n",
    "                       decoder_attention_mask=batch[\"target_mask\"])\n",
    "\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, on_step=True, prog_bar=True, logger=True)\n",
    "        save_log(\n",
    "            self.training_parameters['output_dir'],\n",
    "            self.training_parameters['model_name'],\n",
    "            self.current_epoch,\n",
    "            loss=loss.item(),\n",
    "            data_type='train'\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a validation step, computes loss, and logs the results.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): Batch of validation data.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Loss value for the current batch.\n",
    "        \"\"\"\n",
    "        loss = self.sari_validation_step(batch)\n",
    "        self.log('val_loss', loss, batch_size=self.valid_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def sari_validation_step(self, batch):\n",
    "        \"\"\"\n",
    "        Calculates the SARI score (Summarization Accuracy with Respect to ROUGE) for the validation batch.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): Batch of validation data.\n",
    "\n",
    "        Returns:\n",
    "            float: SARI score for the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        def generate(sentence):\n",
    "            encoding = self.tokenizer(\n",
    "                [sentence],\n",
    "                max_length=self.max_seq_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            input_ids = encoding['input_ids']\n",
    "            attention_mask = encoding['attention_mask']\n",
    "\n",
    "            beam_outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                top_k=120,\n",
    "                top_p=0.95,\n",
    "                early_stopping=True,\n",
    "                num_return_sequences=1\n",
    "            ).to(self.device)\n",
    "\n",
    "            return self.tokenizer.decode(beam_outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        pred_sents = [generate(source) for source in batch[\"source\"]]\n",
    "        score = corpus_sari(batch[\"source\"], pred_sents, [batch[\"targets\"]])\n",
    "        loss = 1 - score / 100\n",
    "        save_log(\n",
    "            self.training_parameters['output_dir'],\n",
    "            self.training_parameters['model_name'],\n",
    "            self.current_epoch,\n",
    "            loss=loss,\n",
    "            sari=score,\n",
    "            data_type='validation'\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing the optimizer and scheduler.\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.adam_epsilon)\n",
    "\n",
    "        # Calculate the total training steps\n",
    "        t_total = (\n",
    "                (\n",
    "                        len(self.train_dataloader().dataset) // self.train_batch_size\n",
    "                ) // self.gradient_accumulation_steps\n",
    "                * float(self.num_train_epochs)\n",
    "        )\n",
    "\n",
    "        if self.scheduler_type == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=t_total\n",
    "            )\n",
    "        else:\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=t_total\n",
    "            )\n",
    "\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step', 'frequency': 1}]\n",
    "\n",
    "    def save_core_model(self):\n",
    "        \"\"\"\n",
    "        Saves the fine-tuned model and tokenizer to the specified directory.\n",
    "        \"\"\"\n",
    "        self.model.save_pretrained(self.model_store_path)\n",
    "        self.tokenizer.save_pretrained(self.model_store_path)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the training DataLoader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The training DataLoader.\n",
    "        \"\"\"\n",
    "        train_dataset = TrainDataset(\n",
    "            data_set_dir=self.data_location,\n",
    "            dataset=self.dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.max_seq_length,\n",
    "            sample_size=self.train_sample_size,\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the validation DataLoader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The validation DataLoader.\n",
    "        \"\"\"\n",
    "        val_dataset = ValDataset(\n",
    "            data_set_dir=self.data_location,\n",
    "            dataset=self.dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.max_seq_length,\n",
    "            sample_size=self.valid_sample_size\n",
    "        )\n",
    "        return DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.valid_batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a169cec-6363-4943-923b-8c0698c07c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keyword_prompting\n",
    "\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# Initialize KeyBERT model\n",
    "keybert_model = KeyBERT()\n",
    "\n",
    "\n",
    "# Function to extract keywords with KeyBERT\n",
    "def extract_keywords(text, top_n=5, diversity=0.5):\n",
    "    \"\"\"\n",
    "    Extracts keywords from a text using KeyBERT.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        top_n (int): Number of top keywords to extract.\n",
    "        diversity (float): Controls the diversity of keywords (0 = low diversity, 1 = high diversity).\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples containing keywords and their scores.\n",
    "    \"\"\"\n",
    "    return keybert_model.extract_keywords(text, top_n=top_n, diversity=diversity)\n",
    "\n",
    "\n",
    "# Function to create prompts using the kw_score strategy\n",
    "def create_kw_score_prompt(text, top_n=5, diversity=0.5):\n",
    "    \"\"\"\n",
    "    Creates a prompt using the kw_score strategy.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        top_n (int): Number of keywords to extract.\n",
    "        diversity (float): Controls the diversity of keywords (0 = low diversity, 1 = high diversity).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt.\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(text, top_n, diversity)\n",
    "    keyword_prompt = \" \".join([f\"{kw[0]}:{kw[1]:.2f}\" for kw in keywords])\n",
    "    return f\"{keyword_prompt} {text}\"\n",
    "\n",
    "\n",
    "# Function to create prompts using the kw_sep strategy\n",
    "def create_kw_sep_prompt(text, top_n=5, diversity=0.5):\n",
    "    \"\"\"\n",
    "    Creates a prompt using the kw_sep strategy.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        top_n (int): Number of keywords to extract.\n",
    "        diversity (float): Controls the diversity of keywords (0 = low diversity, 1 = high diversity).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt.\n",
    "    \"\"\"\n",
    "    keywords = extract_keywords(text, top_n, diversity)\n",
    "    keyword_prompt = \" </s> \".join([kw[0] for kw in keywords]) + \" </s>\"\n",
    "    return f\"{keyword_prompt} {text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86708373-8ff2-4ccd-805b-e9f36b4d79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate_model.simsum_evaluator\n",
    "\n",
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import textstat\n",
    "# from util.processing.preprocessor import get_data_filepath\n",
    "# from util.simsum_models.keyword_prompting import create_kw_sep_prompt, create_kw_score_prompt\n",
    "from easse.sari import corpus_sari as easse_corpus_sari\n",
    "from easse.fkgl import corpus_fkgl as easse_corpus_fkgl\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_dataset(dataset_dir, dataset_name, phase='test'):\n",
    "    \"\"\"\n",
    "    Load the dataset for evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str or Path): Path to the dataset directory.\n",
    "        dataset_name (str): Name of the dataset (e.g., 'dwiki' or 'wiki_doc').\n",
    "        phase (str): Dataset phase to load ('train', 'valid', 'test').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of complex sentences, list of simple sentences)\n",
    "    \"\"\"\n",
    "    complex_filepath = get_data_filepath(dataset_dir, dataset_name, phase, 'complex')\n",
    "    simple_filepath = get_data_filepath(dataset_dir, dataset_name, phase, 'simple')\n",
    "\n",
    "    # Read lines from files\n",
    "    complex_sents = Path(complex_filepath).read_text().splitlines()\n",
    "    simple_sents = Path(simple_filepath).read_text().splitlines()\n",
    "\n",
    "    return complex_sents, simple_sents\n",
    "\n",
    "\n",
    "class SumSimEvaluator:\n",
    "    \"\"\"\n",
    "    A class for evaluating a SumSim-based summarization model using SARI, D-SARI, and FKGL metrics.\n",
    "\n",
    "    Args:\n",
    "        model_config : Configuration dictionary containing the device to run the model on (\"cuda\", \"cpu\", or \"mps\").\n",
    "        summarizer (AutoModelForSeq2SeqLM): Pre-trained summarization model.\n",
    "        simplifier (AutoModelForSeq2SeqLM): Pre-trained simplification model.\n",
    "        summarizer_tokenizer (AutoTokenizer): Tokenizer for the summarization model.\n",
    "        simplifier_tokenizer (AutoTokenizer): Tokenizer for the simplification model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_config, summarizer, simplifier, summarizer_tokenizer, simplifier_tokenizer):\n",
    "        self.summarizer = summarizer.to(model_config['device'])\n",
    "        self.simplifier = simplifier.to(model_config['device'])\n",
    "        self.summarizer_tokenizer = summarizer_tokenizer\n",
    "        self.simplifier_tokenizer = simplifier_tokenizer\n",
    "        self.device = model_config['device']\n",
    "        self.max_seq_length = model_config['max_seq_length']\n",
    "        self.prompting_strategy = model_config.get('prompting_strategy', 'kw_sep')\n",
    "        self.output_location = model_config['output_dir']\n",
    "\n",
    "    def generate_summary(self, sentence, max_length=256):\n",
    "        \"\"\"\n",
    "        Generate a summary for a given input sentence using the summarizer.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence to be summarized.\n",
    "            max_length (int): Maximum length of the generated summary.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated summary.\n",
    "        \"\"\"\n",
    "        inputs = self.summarizer_tokenizer(\n",
    "            sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        summary_ids = self.summarizer.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        return self.summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    def generate_simplified_text(self, source_sent):\n",
    "        \"\"\"\n",
    "        Generate simplified text using the SumSim model with keyword prompting.\n",
    "\n",
    "        Args:\n",
    "            source_sent (str): Source sentence to be simplified.\n",
    "\n",
    "        Returns:\n",
    "            str: Simplified text.\n",
    "        \"\"\"\n",
    "        # Apply keyword prompting based on strategy\n",
    "        if self.prompting_strategy == 'kw_score':\n",
    "            prompt_text = create_kw_score_prompt(source_sent)\n",
    "        else:  # Default to kw_sep\n",
    "            prompt_text = create_kw_sep_prompt(source_sent)\n",
    "\n",
    "        # Generate summary using the summarizer\n",
    "        summary = self.generate_summary(prompt_text)\n",
    "\n",
    "        # Tokenize the summary for simplification\n",
    "        inputs = self.simplifier_tokenizer(\n",
    "            summary,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_seq_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        # Generate simplified output using the simplifier\n",
    "        simplified_ids = self.simplifier.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        return self.simplifier_tokenizer.decode(simplified_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sari_and_d_sari(source_sent, predicted_sent, references):\n",
    "        \"\"\"\n",
    "        Calculate SARI and D-SARI scores for text simplification.\n",
    "\n",
    "        Args:\n",
    "            source_sent (str): Source sentence.\n",
    "            predicted_sent (str): Predicted simplified sentence.\n",
    "            references (list): List of reference simplified sentences.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (SARI score, D-SARI score)\n",
    "        \"\"\"\n",
    "        source_tokens = set(word_tokenize(source_sent))\n",
    "        predicted_tokens = set(word_tokenize(predicted_sent))\n",
    "        reference_tokens = [set(word_tokenize(ref)) for ref in references]\n",
    "\n",
    "        # Calculate addition, deletion, and keep scores\n",
    "        add_scores = [\n",
    "            len(predicted_tokens - ref) / max(1, len(predicted_tokens))\n",
    "            for ref in reference_tokens\n",
    "        ]\n",
    "        keep_scores = [\n",
    "            len(predicted_tokens & ref) / max(1, len(ref))\n",
    "            for ref in reference_tokens\n",
    "        ]\n",
    "        delete_score = len(source_tokens - predicted_tokens) / max(1, len(source_tokens))\n",
    "\n",
    "        sari = (sum(add_scores) + sum(keep_scores) + delete_score) / (len(add_scores) + len(keep_scores) + 1)\n",
    "        d_sari = delete_score  # D-SARI focuses specifically on the deletion component\n",
    "\n",
    "        return sari, d_sari\n",
    "\n",
    "    def calculate_fkgl(self, text):\n",
    "        \"\"\"\n",
    "        Calculate the Flesch-Kincaid Grade Level (FKGL) score.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            float: FKGL score.\n",
    "        \"\"\"\n",
    "        return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    def evaluate(self, source_sentences, reference_sentences):\n",
    "        \"\"\"\n",
    "        Evaluate a set of source and reference sentences using SARI, D-SARI, and FKGL metrics.\n",
    "\n",
    "        Args:\n",
    "            source_sentences (list): List of source sentences to be simplified.\n",
    "            reference_sentences (list): List of corresponding reference sentences.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing average SARI, D-SARI, and FKGL scores.\n",
    "        \"\"\"\n",
    "        total_sari, total_d_sari, total_fkgl = 0, 0, 0\n",
    "        predictions = []\n",
    "        metrics = []\n",
    "\n",
    "        for i, source_sent in enumerate(source_sentences):\n",
    "            try:\n",
    "                predicted_sent = self.generate_simplified_text(source_sent)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating simplified text for sample {i}: {e}\")\n",
    "                predicted_sent = \"\"  # Fallback to an empty prediction\n",
    "\n",
    "            predictions.append(predicted_sent)\n",
    "            references = [reference_sentences[i]]  # Assuming one reference per source\n",
    "\n",
    "            # Calculate SARI and D-SARI scores\n",
    "            sari, d_sari = self.calculate_sari_and_d_sari(source_sent, predicted_sent, references)\n",
    "            total_sari += sari\n",
    "            total_d_sari += d_sari\n",
    "\n",
    "            # Calculate FKGL score\n",
    "            fkgl = self.calculate_fkgl(predicted_sent)\n",
    "            total_fkgl += fkgl\n",
    "\n",
    "            # Calculate EASSE SARI and FKGL for this sample\n",
    "            try:\n",
    "                easse_sari = easse_corpus_sari(orig_sents=[source_sent], sys_sents=[predicted_sent],\n",
    "                                               refs_sents=[references])\n",
    "                easse_fkgl = easse_corpus_fkgl([predicted_sent])\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating EASSE metrics for sample {i}: {e}\")\n",
    "                easse_sari = 0\n",
    "                easse_fkgl = 0\n",
    "\n",
    "            # Print metrics for the sample\n",
    "            print(f\"Sample {i + 1}/{len(source_sentences)}\")\n",
    "            print(f\"Source: {source_sent}\")\n",
    "            print(f\"Predicted: {predicted_sent}\")\n",
    "            print(f\"Reference: {references[0]}\")\n",
    "            print(f\"SARI: {sari:.2f}, D-SARI: {d_sari:.2f}, FKGL: {fkgl:.2f}\")\n",
    "            print(f\"EASSE SARI: {easse_sari:.2f}, EASSE FKGL: {easse_fkgl:.2f}\\n\")\n",
    "\n",
    "            # Store metrics in a dictionary\n",
    "            metrics.append({\n",
    "                'Sample': i + 1,\n",
    "                'Source': source_sent,\n",
    "                'Predicted': predicted_sent,\n",
    "                'Reference': references[0],\n",
    "                'SARI': sari,\n",
    "                'D-SARI': d_sari,\n",
    "                'FKGL': fkgl,\n",
    "                'EASSE SARI': easse_sari,\n",
    "                'EASSE FKGL': easse_fkgl\n",
    "            })\n",
    "\n",
    "        # Calculate average scores\n",
    "        avg_sari = total_sari / len(source_sentences)\n",
    "        avg_d_sari = total_d_sari / len(source_sentences)\n",
    "        avg_fkgl = total_fkgl / len(source_sentences)\n",
    "\n",
    "        # Calculate EASSE SARI and FKGL scores for all predictions\n",
    "        try:\n",
    "            easse_sari = easse_corpus_sari(orig_sents=source_sentences, sys_sents=predictions,\n",
    "                                           refs_sents=[reference_sentences])\n",
    "            easse_fkgl = easse_corpus_fkgl(predictions)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating EASSE metrics for all predictions: {e}\")\n",
    "            easse_sari = 0\n",
    "            easse_fkgl = 0\n",
    "\n",
    "        print(f\"Average SARI: {avg_sari:.2f}\")\n",
    "        print(f\"Average D-SARI: {avg_d_sari:.2f}\")\n",
    "        print(f\"Average FKGL: {avg_fkgl:.2f}\")\n",
    "        print(f\"EASSE SARI: {easse_sari:.2f}\")\n",
    "        print(f\"EASSE FKGL: {easse_fkgl:.2f}\")\n",
    "\n",
    "        # Save metrics to a CSV file using pandas\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df.to_csv('{}/evaluation_metrics_simsum.csv'.format(self.output_location), index=False)\n",
    "\n",
    "        return {\n",
    "            \"SARI\": avg_sari,\n",
    "            \"D-SARI\": avg_d_sari,\n",
    "            \"FKGL\": avg_fkgl,\n",
    "            \"EASSE SARI\": easse_sari,\n",
    "            \"EASSE FKGL\": easse_fkgl\n",
    "        }, df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3123bfc3-4b9b-4f10-8a4e-72697493fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from utils.train\n",
    "# Import relevant libraries\n",
    "import os\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from util.baseline_models.baseline_model import Seq2SeqFineTunedModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Logs validation results at the end of each validation epoch.\n",
    "        \"\"\"\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if hasattr(pl_module, \"is_logger\") and pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(f\"{key} = {metrics[key]}\\n\")\n",
    "                    print(f\"{key}: {metrics[key]}\")\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        \"\"\"\n",
    "        Logs and saves test results to a file at the end of testing.\n",
    "        \"\"\"\n",
    "        logger.info(\"***** Test results *****\")\n",
    "        if hasattr(pl_module, \"is_logger\") and pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            output_file = os.path.join(pl_module.args.output_dir, \"test_results.txt\")\n",
    "            with open(output_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(f\"{key} = {metrics[key]}\\n\")\n",
    "                        writer.write(f\"{key} = {metrics[key]}\\n\")\n",
    "\n",
    "\n",
    "def train(model_config, model_instance=None):\n",
    "    \"\"\"\n",
    "    Function to train the model.\n",
    "\n",
    "    Args:\n",
    "        model_config: Dictionary containing model configurations.\n",
    "        model_instance: Instance of the model to be trained (optional).\n",
    "    \"\"\"\n",
    "    # Seed for reproducibility\n",
    "    seed = model_config.get('seed', 42)\n",
    "    pl.seed_everything(seed)\n",
    "\n",
    "    # Model checkpointing configuration\n",
    "    model_name = model_config.get('model_name')\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=model_config['output_dir'],\n",
    "        filename=f\"{model_name}-checkpoint-{{epoch}}\",\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "        save_top_k=1\n",
    "    )\n",
    "    # Progress bar callback\n",
    "    bar_callback = pl.callbacks.TQDMProgressBar(refresh_rate=1)\n",
    "\n",
    "    # Training parameters\n",
    "    train_params = {\n",
    "        'accumulate_grad_batches': model_config.get('gradient_accumulation_steps', 1),\n",
    "        'max_epochs': model_config.get('num_train_epochs', 5),\n",
    "        'callbacks': [LoggingCallback(), checkpoint_callback, bar_callback],\n",
    "        'logger': TensorBoardLogger(f\"{model_config['output_dir']}/logs\"),\n",
    "        'num_sanity_val_steps': 0\n",
    "    }\n",
    "\n",
    "    # Model initialization (if model instance is not provided)\n",
    "    if model_instance is None:\n",
    "        print(\"Initializing baseline model...\")\n",
    "        model = Seq2SeqFineTunedModel(model_config)\n",
    "    else:\n",
    "        model = model_instance\n",
    "\n",
    "    # Trainer setup and training\n",
    "    trainer = pl.Trainer(**train_params)\n",
    "    print(\"Starting training...\")\n",
    "    trainer.fit(model)\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # Saving the trained model\n",
    "    output_dir = model_config['output_dir']\n",
    "    if \"simsum\" in model_name:\n",
    "        summarizer_save_path = os.path.join(output_dir, f\"{model_name}-summarizer-final\")\n",
    "        simplifier_save_path = os.path.join(output_dir, f\"{model_name}-simplifier-final\")\n",
    "        print(f\"Saving summarizer to {summarizer_save_path}...\")\n",
    "        model.summarizer.save_pretrained(summarizer_save_path)\n",
    "        model.summarizer_tokenizer.save_pretrained(summarizer_save_path)\n",
    "        print(f\"Summarizer saved at {summarizer_save_path}.\")\n",
    "\n",
    "        print(f\"Saving simplifier to {simplifier_save_path}...\")\n",
    "        model.simplifier.save_pretrained(simplifier_save_path)\n",
    "        model.simplifier_tokenizer.save_pretrained(simplifier_save_path)\n",
    "        print(f\"Simplifier saved at {simplifier_save_path}.\")\n",
    "    else:\n",
    "        model_save_path = os.path.join(output_dir, f\"{model_name}-final\")\n",
    "        print(f\"Saving model to {model_save_path}...\")\n",
    "        model.model.save_pretrained(model_save_path)\n",
    "        model.tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"Model saved at {model_save_path}.\")\n",
    "\n",
    "    return model, output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73045fc1-0b09-4261-b2d3-5bd89291d71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04517f33-c3dc-45e9-81fd-43a4ed3763df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from generate_plots\n",
    "\n",
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def identify_files(folder_path):\n",
    "    \"\"\"\n",
    "    Identifies and categorizes files in a specified folder based on\n",
    "    substrings \"_training_log\", \"_validation_log\", and \"_evaluation_metrics\".\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with categorized files, with keys:\n",
    "      'training_log', 'validation_log', and 'evaluation_metrics'.\n",
    "    \"\"\"\n",
    "    categorized_files = {\n",
    "        'training_log': None,\n",
    "        'validation_log': None,\n",
    "        'evaluation_metrics': None\n",
    "    }\n",
    "\n",
    "    # Iterate over files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if \"_training_log\" in file_name:\n",
    "            categorized_files['training_log'] = os.path.join(folder_path, file_name)\n",
    "        elif \"_validation_log\" in file_name:\n",
    "            categorized_files['validation_log'] = os.path.join(folder_path, file_name)\n",
    "        elif \"evaluation_metrics\" in file_name:\n",
    "            categorized_files['evaluation_metrics'] = os.path.join(folder_path, file_name)\n",
    "\n",
    "    return categorized_files\n",
    "\n",
    "\n",
    "def plot_average_loss(output_dir, training_log_path, validation_log_path, output_file='average_loss.csv'):\n",
    "    \"\"\"\n",
    "    Plots the average training and validation loss over epochs, and saves\n",
    "    the averaged loss data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir: Output directory\n",
    "    - training_log_path (str): Path to the training log CSV file.\n",
    "    - validation_log_path (str): Path to the validation log CSV file.\n",
    "    - output_file (str): Path to save the output CSV file containing average loss data.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    training_log = pd.read_csv(training_log_path)\n",
    "    validation_log = pd.read_csv(validation_log_path)\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_training_loss = training_log.groupby('epoch')['loss'].mean().reset_index()\n",
    "    avg_training_loss['data_type'] = 'training'\n",
    "    avg_validation_loss = validation_log.groupby('epoch')['loss'].mean().reset_index()\n",
    "    avg_validation_loss['data_type'] = 'validation'\n",
    "\n",
    "    # Combine the data\n",
    "    combined_loss = pd.concat([avg_training_loss, avg_validation_loss], axis=0)\n",
    "    combined_loss.columns = ['epoch', 'average_loss', 'data_type']\n",
    "\n",
    "    # Save to CSV\n",
    "    combined_loss.to_csv(\"{}/{}\".format(output_dir, output_file), index=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(avg_training_loss['epoch'], avg_training_loss['loss'], marker='o', linestyle='-', color='b', label='Average Training Loss')\n",
    "    plt.plot(avg_validation_loss['epoch'], avg_validation_loss['loss'], marker='x', linestyle='--', color='r', label='Average Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Epoch vs Average Loss (Training and Validation)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"{}/loss.png\".format(output_dir))\n",
    "\n",
    "\n",
    "def plot_metric_distributions(output_dir, evaluation_metrics_path, output_file='average_metrics.csv'):\n",
    "    \"\"\"\n",
    "    Plots the distribution of specified metrics from the evaluation metrics data\n",
    "    and saves the averaged metrics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - output_dir: Output directory\n",
    "    - evaluation_metrics_path (str): Path to the evaluation metrics CSV file.\n",
    "    - output_file (str): Path to save the output CSV file containing average metrics data.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    evaluation_metrics = pd.read_csv(evaluation_metrics_path)\n",
    "    metrics_columns = ['SARI', 'D-SARI', 'FKGL', 'EASSE SARI', 'EASSE FKGL']\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = evaluation_metrics[metrics_columns].mean().reset_index()\n",
    "    avg_metrics.columns = ['metric', 'average_value']\n",
    "    avg_metrics['data_type'] = 'evaluation'\n",
    "\n",
    "    # Save to CSV\n",
    "    avg_metrics.to_csv(\"{}/{}\".format(output_dir, output_file), index=False)\n",
    "\n",
    "    # Plot distributions\n",
    "    for metric in metrics_columns:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        evaluation_metrics[metric].plot(kind='hist', bins=30, alpha=0.7, color='teal', edgecolor='black')\n",
    "        plt.xlabel(metric)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Distribution of {metric}')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"{}/metrics_{}.png\".format(output_dir, metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b2c721-76b6-49a5-b851-502167e831d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Import user-defined libraries\n",
    "# from util.utils import create_experiment_dir, log_parameters\n",
    "# from util.train import train\n",
    "# from util.evaluate_model.simsum_evaluator import SumSimEvaluator\n",
    "# from util.evaluate_model.evaluation_metrics import BartModelEvaluator, load_dataset\n",
    "# from util.simsum_models.simsum_model import SumSimModel\n",
    "# from util.baseline_models.baseline_model import Seq2SeqFineTunedModel\n",
    "# from util.generate_plots import plot_average_loss, plot_metric_distributions, identify_files\n",
    "\n",
    "\n",
    "class ModelRunner:\n",
    "    def __init__(self, configuration):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model configuration.\n",
    "\n",
    "        Args:\n",
    "            configuration: The dictionary containing model configurations\n",
    "        \"\"\"\n",
    "        # Ensure the project root is added to the Python path\n",
    "        # sys.path.append(str(Path(__file__).resolve().parent))\n",
    "\n",
    "        # Initialise the output directory\n",
    "        self.repo_dir = os.getcwd()\n",
    "        self.exp_dir = os.path.join(self.repo_dir,'outputs')\n",
    "        if not os.path.exists(self.exp_dir):\n",
    "            os.makedirs(self.exp_dir)\n",
    "\n",
    "        # Define the model name\n",
    "        self.model_config = configuration.copy()\n",
    "\n",
    "        # Store the model locations\n",
    "        self.model_config['output_dir'] = create_experiment_dir(self.exp_dir)\n",
    "        self.model_config['data_location'] = os.path.join(self.repo_dir,'datasets')\n",
    "        self.model_config['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "        self.model_name = configuration['model_name'].lower()\n",
    "        self.model_save_path = None\n",
    "        self.model = None\n",
    "        self.model_details = None\n",
    "        self.select_model()\n",
    "\n",
    "    def select_model(self):\n",
    "        \"\"\"\n",
    "        Function to select the model class and configure the settings based on the model name.\n",
    "        \"\"\"\n",
    "        if self.model_name == 'bart-baseline':\n",
    "            self.model_config['model_name'] = 'facebook/bart-base' # 'Yale-LILY/brio-cnndm-uncased'\n",
    "            self.model_config['scheduler_type'] = 'linear'\n",
    "        elif self.model_name == 't5-baseline':\n",
    "            self.model_config['model_name'] = 't5-base'\n",
    "            self.model_config['scheduler_type'] = 'cosine'\n",
    "        elif self.model_name == 'bart-simsum':\n",
    "            self.model_config['summarizer_model_name'] = 'ainize/bart-base-cnn'\n",
    "            self.model_config['simplifier_model_name'] = 'facebook/bart-base'\n",
    "            self.model_config['scheduler_type'] = 'cosine'\n",
    "            self.model = SumSimModel(\n",
    "                self.model_config,\n",
    "                summarizer_model_name=self.model_config['summarizer_model_name'],\n",
    "                simplifier_model_name=self.model_config['simplifier_model_name']\n",
    "            )\n",
    "        elif self.model_name == 't5-simsum':\n",
    "            self.model_config['summarizer_model_name'] = 't5-base'\n",
    "            self.model_config['simplifier_model_name'] = 't5-base'\n",
    "            self.model_config['scheduler_type'] = 'cosine'\n",
    "            self.model = SumSimModel(\n",
    "                self.model_config,\n",
    "                summarizer_model_name=self.model_config['summarizer_model_name'],\n",
    "                simplifier_model_name=self.model_config['simplifier_model_name']\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model name. Use 'bart-baseline', 't5-baseline', 'bart-simsum', or 't5-simsum'.\")\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Run the training process.\n",
    "        \"\"\"\n",
    "        # Log training arguments\n",
    "        log_parameters(self.model_config['output_dir'] / \"params.json\", self.model_config)\n",
    "\n",
    "        # Start training\n",
    "        print(\n",
    "            f\"Starting training with {self.model_name.upper()} model on dataset: {self.model_config['dataset']}\"\n",
    "        )\n",
    "        if self.model is None:\n",
    "            # Initialize model if not already set (for baseline models)\n",
    "            self.model = Seq2SeqFineTunedModel(self.model_config)\n",
    "        self.model, self.model_save_path = train(self.model_config, self.model)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Function to evaluate the model.\n",
    "        \"\"\"\n",
    "        print(\"Starting evaluation of models\")\n",
    "        if self.model_name in ['bart-simsum', 't5-simsum']:\n",
    "            evaluator = SumSimEvaluator(self.model_config, self.model.summarizer, self.model.simplifier,\n",
    "                                        self.model.summarizer_tokenizer, self.model.simplifier_tokenizer)\n",
    "        else:\n",
    "            # Use standard evaluator for baseline models\n",
    "            evaluator = BartModelEvaluator(self.model_config, self.model.model, self.model.tokenizer)\n",
    "\n",
    "        # Load datasets (D_Wiki and Wiki_Doc)\n",
    "        dataset_dir = self.model_config['data_location']\n",
    "        dataset_name = self.model_config['dataset']\n",
    "\n",
    "        print(f\"Evaluating on {dataset_name}\")\n",
    "        complex_sents, simple_sents = load_dataset(\n",
    "            dataset_dir, dataset_name, percentage=self.model_config['test_sample_size']\n",
    "        )\n",
    "        scores, score_table = evaluator.evaluate(complex_sents, simple_sents)\n",
    "        print(f\"Results for {dataset_name}: {scores}\")\n",
    "\n",
    "        # Generate plots\n",
    "        files = identify_files(self.model_config['output_dir'])\n",
    "        plot_average_loss(self.model_config['output_dir'], files['training_log'], files['validation_log'])\n",
    "        plot_metric_distributions(self.model_config['output_dir'], files['evaluation_metrics'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa5933-feeb-463a-8631-fbdbd49dfdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BART-BASELINE model on dataset: wiki_doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/abhjha8/ml_projects/Simplifying-Text/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/abhjha8/ml_projects/Simplifying-Text/outputs/exp_1732011189889047 exists and is not empty.\n",
      "\n",
      "  | Name  | Type                         | Params | Mode\n",
      "--------------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 139 M  | eval\n",
      "--------------------------------------------------------------\n",
      "139 M     Trainable params\n",
      "0         Non-trainable params\n",
      "139 M     Total params\n",
      "557.682   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "182       Modules in eval mode\n",
      "/Users/abhjha8/ml_projects/Simplifying-Text/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Initializing TrainDataset...\n",
      "Dataset paths initialized.\n",
      "Initializing TrainDataset...\n",
      "Dataset paths initialized.\n",
      "Initializing ValDataset...\n",
      "Dataset paths initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhjha8/ml_projects/Simplifying-Text/venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/Users/abhjha8/ml_projects/Simplifying-Text/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd906ddd5361466eb04f1dadcc074f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93223a38d444ea9a561994d7f665494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Initialise user defined libraries\n",
    "# from model_runner import ModelRunner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Run the main functions to summarize text\n",
    "    \"\"\"\n",
    "    configuration = {\n",
    "        'seed': 0,\n",
    "        'model_name': 'bart-baseline',\n",
    "        'dataset': 'wiki_doc',\n",
    "        'num_train_epochs': 3,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'train_batch_size': 4,\n",
    "        'valid_batch_size': 4,\n",
    "        'learning_rate': 1e-5,\n",
    "        'max_seq_length': 256,\n",
    "        'adam_epsilon': 1e-8,\n",
    "        'weight_decay': 0.0001,\n",
    "        'warmup_steps': 5,\n",
    "        'custom_loss': True,\n",
    "        'train_sample_size': 0.001,\n",
    "        'valid_sample_size': 0.01,\n",
    "\n",
    "        # for simsum models\n",
    "        'lambda_': 0.001,\n",
    "        'hidden_size': 1,\n",
    "        'w1': 1,\n",
    "        'prompting_strategy': 'kw_score',\n",
    "        'div_score': 0.9,\n",
    "        'top_keywords': 5,\n",
    "        'test_sample_size': 0.005\n",
    "    }\n",
    "\n",
    "    # Initialize, run and evaluate the model\n",
    "    model = ModelRunner(configuration)\n",
    "    model.train_model()\n",
    "    model.evaluate_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
